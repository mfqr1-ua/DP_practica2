# -*- coding: utf-8 -*-
"""Copia de peer-assessment_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16_WGX27mw4CY-t3ee2zkzlyGJz-FZuIL

<center><img src="https://images.twinkl.co.uk/tw1n/image/private/t_630/image_repo/b8/06/t-lf-242-pupil-voice-learning-child-led-learning-peer-assessment-cards_ver_1.jpg" height="100"></center>

# Detección de incoherencias en evaluación por pares

Profesor: Juan Ramón Rico (<juanramonrico@ua.es>)

## Descripción
---
No cabe duda que las redes neuronales han avanzado en tareas donde se usa texto y valores numéricos. Concretametne en las encuestas o en la evaluación por pares es habitual encontrar valores numéricos (libres o sujetos a una escala - Likert) y comentarios de los evaluadores respecto a las secciones evaluadas en forma de texto (feedback).

- Artículo sobre evaluación por pares donde se han usado redes recurrentes entre otras metodologías para detectar incongruencias en las respuestas <https://www.sciencedirect.com/science/article/pii/S0360131519301629?dgcid=author> ha servido como base para esta actividad.
---
"""

import warnings
warnings.filterwarnings('ignore', category=UserWarning)

"""# Introducción

El uso de la evaluación por pares para actividades abiertas tiene ventajas tanto para los profesores como para los estudiantes. Los profesores pueden reducir la carga de trabajo del proceso de corrección y los estudiantes logran una mejor comprensión de la materia al evaluar las actividades de sus compañeros. Para facilitar el proceso, es aconsejable proporcionar a los estudiantes una rúbrica sobre la cual realizar la evaluación de sus compañeros; sin embargo, limitarse a proporcionar sólo puntuaciones numéricas es perjudicial, ya que impide proporcionar una retroalimentación valiosa a otros compañeros. Dado que esta evaluación produce dos modalidades de la misma evaluación, a saber, la puntuación numérica y la retroalimentación textual, es posible aplicar técnicas automáticas para detectar inconsistencias en la evaluación, minimizando así la carga de trabajo de los profesores para supervisar todo el proceso.

Esta actividad estará enfocada en solo una parte de la detección de incongruencias que será la predicción de calificación de una sección usando únicamente información textual.

# Conjunto de datos

Los datos que vamos a utilizar para esta se pueden descargar en un archivo tipo CSV donde figura:

- `activity`: es la actividad desarrollada, en este caso 1 o 2. Son actividades independientes y los más lógico es entrenar modelos (redes neuronales) independientes para cada una;
-  `year`:  año de comienzo del curso académico estudiado;
- `group`: con valores de 1 si es de mañana o 2 si es de tarde;
- `evaluator`: identificador interno del evaluador para una actividad concreta (la actividad 1 o la 2, de forma excluyente);
- `work` : es el identificador del trabajo. La entrega se realizaba mediante una URL por lo que hay ocasiones en la que es privada o no accesible y no se ha podido evaluar.
- `secction`: número de sección que se evalua dentro de cada actividad, o 'grade1', 'grade2' cuando se trata de la nota final del trabajo evaluado.
- `value`: valor numérico comprendido entre 0 y 3. Siendo 0 no realizado o realizado incorrectamente y 3 realizado correctamente. Este número puede tener decimales debido a que corresponde al promedio de los valores de la sección.
- `feedback`: texto libre correspondiente a las recomendaciones que el evaluador realiza en cada sección. Puede estar en blanco lo que indica que no se realizan comentarios y corresponde a que todo está correcto.

Para esta actividad necesitaremos `activity`, `value` y `feeback`.
"""

import numpy as np
import pandas as pd

data = pd.read_csv('https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz', sep='\t', decimal=',')
data.fillna('', inplace=True) # Reemplazar los valores en blanco por cadena vacías
data = data.sample(frac=1, random_state=123) # Barajar los ejemplos de entrada (filas de la tabla)
display(data.head())
data.dtypes

"""El atributo `section` contiene los identificadores de las diferentes secciones, así como las calificación final (`grade1` y `grade2` cuyos valores están entre 0 y 10) según la actividad. Esta actividad se tiene que evaluar por separado para la actividad 1 o 2 usando únicamente las secciones numéricas (1,2,3,4,5,6 y 7) cuyos valores oscilan entre 0 y 3."""

np.sort(data['section'].unique())

"""# Visualizar las valoraciones"""

data[data['section'].isin(['1','2','3','4','5','6','7'])][['activity','value']].groupby('activity').hist()

"""Podemos ver como la mayoría de valores son cercanos a 3. No obstante tenemos que predecir cuando este valor va disminuyendo atendiendo a las palabras usadas en el contexto restringido de cada actividad.

# Preprocesar el texto

Es necesario preprocesar el texto para descartar símbolos de puntuación, valorar igualmente a palabras en mayúscula y minúscula y extraeer la raiz de las palabras (lemas) para procesarlas como iguales.

Para ello usaremos bibliotecas de procesamiento de lenguaje natural (PLN).
"""

import nltk
nltk.download('stopwords')

from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')
stemmer = SnowballStemmer("spanish",ignore_stopwords=True)

preprocessed_feedback = []
for i in data.feedback:
  tokens = [stemmer.stem(word) for word in tokenizer.tokenize(i.lower())]
  preprocessed_feedback.append(np.array(' '.join(tokens)))

data['feedback prep'] = preprocessed_feedback
data['feedback prep'] = data['feedback prep'].astype('str')
data.head()

data.dtypes

"""# Convertir datos de entrenamiento a la forma correcta

En este caso los datos de tipo texto hay transformarlos en secuencias de número que recibirá la red neuronal.

- En el siguiente enlace <https://www.programcreek.com/python/example/106871/keras.preprocessing.text.Tokenizer> hay varios ejemplos de como convertir el texto que ya tenemos preprocesado a secuencias de números;
- De la secuencia de números hay que aplicar `pad_sequences` como truco para igualar la longitud de todas las secuencias de entreada. Nos facilita la tarea de entrenar un red recurrente con `Keras`.
"""

# Seleccionar la actividad 1 y sus secciones, ya que se evaluan por separado. La actividad 2 se seleccionaría por separado
new_data = data[(data['activity']==1) & data['section'].isin(['1','2','3','4','5','6','7'])]

X = ... # Valores de new_data['feedback prep'] con el padding aplicado
y = new_data['value'].values

"""# Creando la red neuronal

Se han preprocesado los datos y los hemos convertido al formato correcto. Ahora tenemos que diseñar nuestra red para entrenarla y realizar las predicciones.

En el caso de codificar la entrada como un conjunto de palabras (Bag of Words) tendremos un vector de entrada con todas las palabras posibles del vocabulario. Cada posición del vector representa si está presente la palabra o no, el número de veces que se repite, o el valor TF-IDF correspondiente. La dimensión incial de la red cuando entrena será de `(batch_size, vocabulary_len)`.

Otro caso diferente corresponde a una codificación del la entrada atendiendo a una secuencia. Cada valor (entero) del vector de entrada representa a una palabra y la entrada cumple con estas tres dimensiones `(batch_size, time_steps, seq_len)` que necesita una red recurrente.
"""

#Carga y preprocesado básico

import numpy as np
import pandas as pd
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import RegexpTokenizer

# Descargar stopwords y configurar tokenizer+stemmer
nltk.download('stopwords')
stemmer = SnowballStemmer("spanish", ignore_stopwords=True)
tokenizer_regex = RegexpTokenizer(r'\w+')

#Leer y barajar el CSV
data = pd.read_csv(
    'https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz',
    sep='\t', decimal=','
)
data.fillna('', inplace=True)
data = data.sample(frac=1, random_state=123).reset_index(drop=True)

#Tokenización, lower y stemming
preprocessed = []
for fb in data['feedback']:
    tokens = tokenizer_regex.tokenize(fb.lower())
    stems  = [stemmer.stem(t) for t in tokens]
    preprocessed.append(" ".join(stems))

data['feedback_prep'] = preprocessed

print("Primeras filas tras preprocesado:")
display(data[['feedback', 'feedback_prep', 'value']].head())

# ────────── Bloque 2: División entrenamiento / test ──────────

from sklearn.model_selection import train_test_split

X = data['feedback_prep'].values
y = data['value'].values

# 75% entrenamiento, 25% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.25,
    random_state=42,
    shuffle=True
)

print(f"Tamaño train: {len(X_train)}, tamaño test: {len(X_test)}")

#BoW-TFIDF + MLP con input dinámico

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

# 1) Vectorización TF–IDF
from nltk.corpus import stopwords
spanish_stop = stopwords.words('spanish')

tfidf = TfidfVectorizer(
    max_features=5000,      # límite superior
    lowercase=True,
    stop_words=spanish_stop
)
X_train_bow = tfidf.fit_transform(X_train).toarray()
X_test_bow  = tfidf.transform(X_test).toarray()

#Leemos el número real de features
n_features = X_train_bow.shape[1]
print(f"TF–IDF real vocab size: {n_features}")

#Definir y compilar MLP usando dimensión dinámica
model_bow = Sequential([
    Input(shape=(n_features,)),    # <-- aquí usamos n_features en lugar de 5000 fijo
    Dense(64, activation='relu'),
    Dense(1)                       # salida continua 0–3
])
model_bow.compile(
    optimizer='adam',
    loss='mean_absolute_error',
    metrics=['mae']
)

#Entrenar
model_bow.fit(
    X_train_bow, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.1,
    verbose=2
)

#Evaluar
y_pred_bow = model_bow.predict(X_test_bow).ravel()
mae_bow    = mean_absolute_error(y_test, y_pred_bow)
print(f"\nBoW+MLP — Test MAE: {mae_bow:.4f}")

#Secuencia + Embedding + LSTM

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Input

#Tokenizer y conversión a secuencias de índices
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)
seqs_train = tokenizer.texts_to_sequences(X_train)
seqs_test  = tokenizer.texts_to_sequences(X_test)

#Rellenar pad hasta la longitud máxima de train
maxlen     = max(len(s) for s in seqs_train)
vocab_size = len(tokenizer.word_index) + 1

X_train_seq = pad_sequences(seqs_train, maxlen=maxlen, padding='post')
X_test_seq  = pad_sequences(seqs_test,  maxlen=maxlen, padding='post')

#Definir y compilar LSTM
model_seq = Sequential([
    Input(shape=(maxlen,)),
    Embedding(input_dim=vocab_size, output_dim=100, input_length=maxlen),
    LSTM(64),
    Dense(1)
])
model_seq.compile(
    optimizer='adam',
    loss='mean_absolute_error',
    metrics=['mae']
)

#Entrenar
model_seq.fit(
    X_train_seq, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.1,
    verbose=2
)

#Evaluar
y_pred_seq = model_seq.predict(X_test_seq).ravel()
mae_seq    = mean_absolute_error(y_test, y_pred_seq)
print(f"\nSeq+LSTM — Test MAE: {mae_seq:.4f}")

#Validación Cruzada 10-Fold para el BoW+MLP

from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
import numpy as np
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Input, Dense

#Concatenar train + test para aplicar CV sobre todo el conjunto
X_all_bow = np.vstack([X_train_bow, X_test_bow])
y_all      = np.concatenate([y_train,       y_test      ])

#Definir KFold
kf = KFold(n_splits=10, shuffle=True, random_state=42)

mae_scores = []

for fold, (train_idx, val_idx) in enumerate(kf.split(X_all_bow), start=1):
    # Limpiar estado de TF para no mezclar pesos
    tf.keras.backend.clear_session()

    #Reconstruir el modelo BoW-MLP idéntico al Bloque 3A
    model_cv = Sequential([
        Input(shape=(n_features,)),
        Dense(64, activation='relu'),
        Dense(1)
    ])
    model_cv.compile(
        optimizer='adam',
        loss='mean_absolute_error'
    )

    #Entrenar en este fold
    model_cv.fit(
        X_all_bow[train_idx], y_all[train_idx],
        epochs=10,
        batch_size=32,
        verbose=0
    )

    #Evaluar MAE en la partición de validación
    y_pred = model_cv.predict(X_all_bow[val_idx]).ravel()
    mae    = mean_absolute_error(y_all[val_idx], y_pred)
    mae_scores.append(mae)
    print(f"Fold {fold:2d} — MAE: {mae:.4f}")

#Resultados globales
mae_mean = np.mean(mae_scores)
mae_std  = np.std(mae_scores)
print(f"\nMAE 10-CV mean: {mae_mean:.4f} ± {mae_std:.4f}")

# Filtrar datos y extraer variables
filtered = data[ data['section'].isin(['1','2','3','4','5','6','7']) ].reset_index(drop=True)

# Texto preprocesado, valor y actividad
X_text   = filtered['feedback_prep'].tolist()   # lista de strings
y        = filtered['value'].values             # valores 0–3
activity = filtered['activity'].values          # 1 o 2

#Vectorización TF-IDF para BoW/TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf     = TfidfVectorizer(max_features=5000)              # límite a 5 000 términos
X_bow     = tfidf.fit_transform(X_text).toarray()           # (n_samples, 5000)

#Split incluyendo 'activity'
from sklearn.model_selection import train_test_split

X_train_bow, X_test_bow, \
y_train, y_test, \
activity_train, activity_test = train_test_split(
    X_bow,
    y,
    activity,
    test_size=0.25,
    random_state=42,
    stratify=activity
)

#Definición del modelo BoW+MLP
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense, InputLayer

input_dim = X_train_bow.shape[1]

model_bow = Sequential([
    InputLayer(input_shape=(input_dim,)),
    Dense(128, activation='relu'),
    Dense(64,  activation='relu'),
    Dense(1,   activation='linear')
])
model_bow.compile(
    optimizer='adam',
    loss='mean_absolute_error',
    metrics=['mae']
)
model_bow.summary()

#Entrenamiento del modelo BoW+MLP
history = model_bow.fit(
    X_train_bow, y_train,
    epochs=10,
    batch_size=32,
    validation_split=0.1,
    verbose=2
)

#Evaluación por Actividad (1 y 2)
from sklearn.metrics import mean_absolute_error
import numpy as np

for act in [1, 2]:
    # 1) Filtrar test-set por actividad
    idx   = np.where(activity_test == act)[0]
    X_act = X_test_bow[idx]
    y_act = y_test[idx]

    # 2) Predecir y calcular MAE
    y_pred_act = model_bow.predict(X_act).ravel()
    mae_act    = mean_absolute_error(y_act, y_pred_act)

    print(f"Actividad {act} — MAE BoW+MLP: {mae_act:.4f}")

#Estudio Comparativo con Test de Wilcoxon usando KFold
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error
from scipy.stats import wilcoxon
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer, Dense, Embedding, GlobalAveragePooling1D
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

#Construir secuencias + padding
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_text)
seqs       = tokenizer.texts_to_sequences(X_text)
maxlen     = max(len(s) for s in seqs)
vocab_size = len(tokenizer.word_index) + 1
X_seq      = pad_sequences(seqs, maxlen=maxlen, padding='post')  # (n_samples, maxlen)

# ---- 2) Definir funciones constructoras de modelos ----
def build_bow_model(input_dim):
    m = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(128, activation='relu'),
        Dense(64,  activation='relu'),
        Dense(1,   activation='linear')
    ])
    m.compile(optimizer='adam', loss='mean_absolute_error')
    return m

def build_seq_model(maxlen, vocab_size, embedding_dim=100):
    m = Sequential([
        InputLayer(input_shape=(maxlen,)),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=maxlen),
        GlobalAveragePooling1D(),
        Dense(64, activation='relu'),
        Dense(1,  activation='linear')
    ])
    m.compile(optimizer='adam', loss='mean_absolute_error')
    return m

# ---- 3) KFold 10-CV para recoger MAEs ----
kf      = KFold(n_splits=10, shuffle=True, random_state=42)
mae_bow = []
mae_seq = []

for train_idx, val_idx in kf.split(X_bow):
    Xb_tr, Xb_va = X_bow[train_idx], X_bow[val_idx]
    Xs_tr, Xs_va = X_seq[train_idx], X_seq[val_idx]
    y_tr,  y_va  = y[train_idx],   y[val_idx]

    # BoW model
    m1 = build_bow_model(input_dim=Xb_tr.shape[1])
    m1.fit(Xb_tr, y_tr, epochs=5, batch_size=32, verbose=0)
    pred1 = m1.predict(Xb_va).ravel()
    mae_bow.append(mean_absolute_error(y_va, pred1))

    # Seq model
    m2 = build_seq_model(maxlen=maxlen, vocab_size=vocab_size)
    m2.fit(Xs_tr, y_tr, epochs=5, batch_size=32, verbose=0)
    pred2 = m2.predict(Xs_va).ravel()
    mae_seq.append(mean_absolute_error(y_va, pred2))

#Test de Wilcoxon pareado
stat, p_value = wilcoxon(mae_bow, mae_seq)
print(f"MAE BoW folds: {np.array(mae_bow).round(3)}")
print(f"MAE Seq folds: {np.array(mae_seq).round(3)}\n")
print("Wilcoxon signed‐rank test:")
print(f"  statistic = {stat:.3f}, p‐value = {p_value:.3f}")
if p_value < 0.05:
    print("=> Diferencia estadísticamente significativa (p < 0.05)")
else:
    print("=> No hay evidencia suficiente para afirmar diferencia (p ≥ 0.05)")

"""NIVEL MEDIO"""

# Aplicar más arquitecturas de redes distintas a la del apartado básico;


import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import InputLayer, Dense, Dropout, BatchNormalization
from tensorflow.keras.layers import Embedding, LSTM, GlobalAveragePooling1D


# Partir en train/test
Xb_tr, Xb_te, Xs_tr, Xs_te, y_tr, y_te = train_test_split(
    X_bow, X_seq, y,
    test_size=0.25, random_state=42, shuffle=True
)

# Definiciones de las nuevas arquitecturas

def build_bow_deep(input_dim):
    """MLP más profundo para BoW/TF-IDF con normalización y dropout."""
    m = Sequential([
        InputLayer(input_shape=(input_dim,)),
        Dense(256, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(128, activation='relu'),
        BatchNormalization(),
        Dropout(0.5),
        Dense(1, activation='linear')
    ])
    m.compile(optimizer='adam', loss='mean_absolute_error')
    return m

def build_seq_lstm(maxlen, vocab_size, embed_dim=100):
    """Red recurrente LSTM sobre secuencias de índices."""
    m = Sequential([
        InputLayer(input_shape=(maxlen,)),
        Embedding(input_dim=vocab_size, output_dim=embed_dim, input_length=maxlen),
        LSTM(64),
        Dense(32, activation='relu'),
        Dense(1, activation='linear')
    ])
    m.compile(optimizer='adam', loss='mean_absolute_error')
    return m

# Construir, entrenar y evaluar

# BoW profundo
model_bow_deep = build_bow_deep(input_dim=Xb_tr.shape[1])
model_bow_deep.fit(
    Xb_tr, y_tr,
    epochs=10, batch_size=32,
    validation_split=0.1, verbose=2
)
pred_bow_deep = model_bow_deep.predict(Xb_te).ravel()
mae_bow_deep  = mean_absolute_error(y_te, pred_bow_deep)
print(f"\nMAE BoW profundo: {mae_bow_deep:.4f}")

# LSTM sobre secuencias
maxlen    = Xs_tr.shape[1]
vocab_size = np.max(Xs_tr) + 1  # asumimos que tus secuencias ya usan índices de 1..vocab
model_seq_lstm = build_seq_lstm(maxlen, vocab_size)
model_seq_lstm.fit(
    Xs_tr, y_tr,
    epochs=10, batch_size=32,
    validation_split=0.1, verbose=2
)
pred_seq_lstm = model_seq_lstm.predict(Xs_te).ravel()
mae_seq_lstm  = mean_absolute_error(y_te, pred_seq_lstm)
print(f"MAE LSTM secuencial: {mae_seq_lstm:.4f}")

# ────────── Bloque 0: Instalar Java 17 ──────────
# (comentado para evitar llamadas a shell en un script)
# !apt-get update -qq
# !apt-get install -y -qq openjdk-17-jdk-headless

# Verificamos la versión de Java
# !java -version

# Bloque Avanzado (Corregido): Preprocesado + Augmentación

import pandas as pd
import numpy as np
import random

# Carga y shuffle
data = pd.read_csv(
    'https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz',
    sep='\t', decimal=','
)
data.fillna('', inplace=True)
data = data.sample(frac=1, random_state=42).reset_index(drop=True)

# Descarga recursos
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
from nltk.corpus import stopwords, wordnet
from nltk.tokenize import RegexpTokenizer
from nltk.stem import SnowballStemmer

# Stopwords
custom_stop = {'foto','imagen','etc'}
all_stop    = set(stopwords.words('spanish')) | custom_stop

# Tokenizer & Stemmer
tokenizer_regex = RegexpTokenizer(r'\w+')
stemmer         = SnowballStemmer("spanish")

# Función de sinónimos
def synonym_augment(tokens, p=0.2):
    out = []
    for t in tokens:
        if random.random() < p:
            syns = []
            for syn in wordnet.synsets(t, lang='spa'):
                for lemma in syn.lemmas('spa'):
                    name = lemma.name().replace('_',' ')
                    if name != t:
                        syns.append(name)
            out.append(random.choice(syns) if syns else t)
        else:
            out.append(t)
    return out

# Paso 1: básico (stemming + sinónimos)
basic_texts = []
for fb in data['feedback']:
    toks = tokenizer_regex.tokenize(fb.lower())
    toks = [t for t in toks if t not in all_stop]
    toks = [stemmer.stem(t) for t in toks]
    toks = synonym_augment(toks, p=0.2)
    basic_texts.append(" ".join(toks))

data['feedback_basic'] = basic_texts

# Función de mezcla (usa basic_texts completo)
def mix_with_peer_basic(tokens, idx, basic_list, values, p=0.3):
    if random.random() > p:
        return tokens
    same = [i for i,v in enumerate(values) if v==values[idx] and i!=idx]
    if not same:
        return tokens
    peer = random.choice(same)
    peer_toks = basic_list[peer].split()
    cut = len(tokens)//2
    return tokens[:cut] + peer_toks[cut:]

# Paso 2: mezcla en base al basic_texts completo
advanced_texts = []
values = data['value'].tolist()

for idx, basic in enumerate(basic_texts):
    toks = basic.split()
    toks = mix_with_peer_basic(toks, idx, basic_texts, values, p=0.3)
    advanced_texts.append(" ".join(toks))

data['feedback_adv'] = advanced_texts

# Ejemplos
for orig, basic, adv in zip(data['feedback'].head(5),
                            data['feedback_basic'].head(5),
                            data['feedback_adv'].head(5)):
    print("ORIGINAL:", orig)
    print("BÁSICO   :", basic)
    print("AVANZADO :", adv)
    print("-"*60)

# Ajuste del algoritmo de optimización

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import EarlyStopping

# — 0) Asumimos que YA tienes:
#    seqs      = lista de secuencias numéricas (cada secuencia len ≤ maxlen)
#    maxlen    = longitud de secuencia (int)
#    vocab_size= tamaño de vocabulario + 1 (int)
#    y         = np.array de valores continuos (shape = n_samples,)

# — 1) Convertir a array y dividir en train/test
X_seq = tf.keras.preprocessing.sequence.pad_sequences(
    seqs, maxlen=maxlen, padding='post'
)  # shape = (n_samples, maxlen)

X_train_seq, X_test_seq, y_train, y_test = train_test_split(
    X_seq, y, test_size=0.25, random_state=42
)

# — 2) Constructor de tu modelo de secuencias
def build_seq_model(maxlen, vocab_size, embedding_dim=100):
    model = tf.keras.Sequential([
        tf.keras.layers.Input(shape=(maxlen,)),
        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        tf.keras.layers.GlobalAveragePooling1D(),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1, activation='linear')
    ])
    return model

# — 3) Definir los optimizadores a comparar
optimizers = {
    "Adam":  tf.keras.optimizers.Adam(learning_rate=1e-3),
    "SGD":   tf.keras.optimizers.SGD(learning_rate=1e-2, momentum=0.9),
    "AdamW": tf.keras.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5),
}

# — 4) Parada temprana (EarlyStopping) sobre val_loss
es = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)

# — 5) Entrenar y evaluar cada optimizador
results = {}
for name, opt in optimizers.items():
    print(f"\n>>> {name} — Entrenando")
    model = build_seq_model(maxlen, vocab_size)
    model.compile(
        optimizer=opt,
        loss="mean_absolute_error",
        metrics=["mae"]
    )
    hist = model.fit(
        X_train_seq, y_train,
        epochs=20,
        batch_size=32,
        validation_split=0.1,
        callbacks=[es],
        verbose=2
    )
    test_mae = model.evaluate(X_test_seq, y_test, verbose=0)[1]
    stopped_epoch = len(hist.history["loss"])
    results[name] = (test_mae, stopped_epoch)
    print(f"→ {name}: Test MAE = {test_mae:.4f}, detenido en epoch {stopped_epoch}")

# — 6) Resumen final
print("\nResumen comparativo de optimizadores:")
for name, (mae, ep) in results.items():
    print(f"• {name:5s} → MAE {mae:.4f}, epochs {ep}")

import pandas as pd
import numpy as np
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import RegexpTokenizer

# 1) Carga y limpieza
data = pd.read_csv('https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz',
                   sep='\t', decimal=',')
data.fillna('', inplace=True)
data = data.sample(frac=1, random_state=123).reset_index(drop=True)

# 2) Preprocesado de texto
nltk.download('stopwords')
tokenizer = RegexpTokenizer(r'\w+')
stemmer   = SnowballStemmer("spanish", ignore_stopwords=True)

def preprocess_text(txt):
    toks = tokenizer.tokenize(txt.lower())
    return " ".join(stemmer.stem(t) for t in toks)

data['feedback_prep'] = data['feedback'].map(preprocess_text)

# Ahora filtramos para la actividad 1 y las secciones numéricas
new_data = data[
    (data['activity']==1) &
    data['section'].isin(['1','2','3','4','5','6','7'])
].reset_index(drop=True)

# comprobación rápida
print(new_data.columns)

#Uso de redes ya entrenadas para aplicarlas directamente al texto

import pandas as pd
import numpy as np
import nltk
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import RegexpTokenizer
import tensorflow as tf
import tensorflow_hub as hub
from sklearn.model_selection import train_test_split

#Leer y limpiar datos
data = pd.read_csv(
    'https://www.dlsi.ua.es/~juanra/UA/dataset/dcadep/dcadep_melt_grades.csv.gz',
    sep='\t', decimal=','
)
data.fillna('', inplace=True)
data = data.sample(frac=1, random_state=123).reset_index(drop=True)

# Preprocesado de texto: tokenización + stemming
nltk.download('stopwords', quiet=True)
tokenizer = RegexpTokenizer(r'\w+')
stemmer   = SnowballStemmer("spanish", ignore_stopwords=True)
def preprocess_text(txt):
    toks = tokenizer.tokenize(txt.lower())
    return " ".join(stemmer.stem(t) for t in toks)
data['feedback_prep'] = data['feedback'].map(preprocess_text)

#Filtrar sólo actividad 1 y secciones '1'..'7'
new_data = data[
    (data['activity']==1) &
    data['section'].isin([str(i) for i in range(1,8)])
].reset_index(drop=True)

# Arrays de entrada y salida
texts  = new_data['feedback_prep'].values
values = new_data['value'].values

# train/test split
X_train_txt, X_test_txt, y_train, y_test = train_test_split(
    texts, values,
    test_size=0.25,
    random_state=42,
    shuffle=True
)

# Cargar el USE con hub.load
MODULE_URL = "https://tfhub.dev/google/universal-sentence-encoder/4"
use_encoder = hub.load(MODULE_URL)

# Montar el modelo usando una capa Lambda
text_input = tf.keras.Input(shape=(), dtype=tf.string, name="text_input")
embed = tf.keras.layers.Lambda(lambda x: use_encoder(x),
                               output_shape=(512,),
                               name="USE_embedding")(text_input)
x = tf.keras.layers.Dense(64, activation="relu")(embed)
output = tf.keras.layers.Dense(1, activation="linear")(x)

model_use = tf.keras.Model(inputs=text_input, outputs=output, name="USE_regressor")
model_use.compile(
    optimizer="adam",
    loss="mean_absolute_error",
    metrics=["mae"]
)
model_use.summary()

# Entrenar
model_use.fit(
    X_train_txt, y_train,
    validation_split=0.1,
    epochs=10,
    batch_size=32,
    verbose=2
)

# Evaluar
test_loss, test_mae = model_use.evaluate(
    X_test_txt, y_test,
    batch_size=32,
    verbose=0
)
print(f"\nTest MAE (USE): {test_mae:.4f}")

# ────────── Bloque: Explicación con SHAP (simplificado) ──────────

# 1) Importar SHAP (si está disponible)
try:
    import shap
    shap.initjs()
except ImportError:
    shap = None

# 2) Usar 100 ejemplos aleatorios de train como fondo
if shap is not None:
    background = X_train_txt[np.random.choice(len(X_train_txt), 100, replace=False)]

    # 3) Crear el Explainer dejando que SHAP infiera el masker
    explainer = shap.Explainer(model_use, background)

    # 4) Explicar unas cuantas muestras de test
    samples     = X_test_txt[:3]
    shap_values = explainer(samples)

    # 5) Mostrar contribuciones palabra a palabra
    for i, sv in enumerate(shap_values):
        print(f"\n— Explicación muestra #{i} —")
        print(samples[i], "\n")
        shap.plots.text(sv)
else:
    print("El paquete shap no está instalado. No se pueden generar explicaciones.")

# ────────── Bloque: Comparativa con test estadístico ──────────

pred_bow = model_bow.predict(X_test_bow).ravel()
pred_use = model_use.predict(X_test_txt).ravel()

errors_bow = np.abs(y_test - pred_bow)
errors_use = np.abs(y_test - pred_use)

from scipy.stats import wilcoxon
stat, p_value = wilcoxon(errors_bow, errors_use)

print("\nComparativa BoW vs USE en test:")
print(f"MAE BoW: {np.mean(errors_bow):.4f} — MAE USE: {np.mean(errors_use):.4f}")
print(f"Wilcoxon statistic = {stat:.4f}, p-value = {p_value:.4f}")
if p_value < 0.05:
    print("=> Diferencia estadísticamente significativa (p < 0.05)")
else:
    print("=> No hay evidencia suficiente para afirmar diferencia (p ≥ 0.05)")
